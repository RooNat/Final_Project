{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# LSTM Model for price prediction"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import datetime\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, explained_variance_score, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import keras.utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "import math\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from sklearn.metrics import classification_report"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "outputs": [],
   "source": [
    "def rename_datetime(data):\n",
    "    data.rename(columns={'Unnamed: 0': 'datetime'},inplace=True)\n",
    "    data['datetime']=pd.to_datetime(data['datetime'])\n",
    "    data.set_index('datetime', inplace=True)\n",
    "    return data\n",
    "# feature selection\n",
    "# create features\n",
    "def create_features(data, g_lag, tv_lag, tw_lag,v_lag):\n",
    "    data['return'] =data['Close'].pct_change().dropna()\n",
    "    data['google_trends_lag']=data['google_trends'].shift(g_lag)\n",
    "    data['tweet_volume_lag']=data['tweet_volume'].shift(tv_lag)\n",
    "    data['tw_polarity_lag'] = data['tw_polarity'].shift(tw_lag)\n",
    "    data['volume_lag'] = data['Volume'].shift(v_lag)\n",
    "\n",
    "    # data.drop(columns=['Open','High','Low'],inplace=True)\n",
    "    # Add features like RSI? Moving average?\n",
    "\n",
    "    data.dropna(inplace=True)\n",
    "    return data\n",
    "\n",
    "# keep the wanted features\n",
    "def keep_features(feature_conditions):\n",
    "    features=['return']\n",
    "    for feature, condition in feature_conditions.items():\n",
    "        if condition:\n",
    "            features.append(feature)\n",
    "    return features\n",
    "\n",
    "\n",
    "def timestep_matrix(dataset,label, timestep):\n",
    "    X_data, y_data=[], []\n",
    "    for i in range(len(dataset)-timestep-1):\n",
    "        feature=dataset[i:(i+timestep)]\n",
    "        X_data.append(feature)\n",
    "        y_data.append(label[i+timestep])\n",
    "    y_data=np.array(y_data).reshape(len(y_data),1)\n",
    "    return np.array(X_data), y_data\n",
    "\n",
    "# normalize data using Minmaxscaler\n",
    "def normalize_reshape_data(train, test,val,timestep):\n",
    "    train_y=train['return'].to_numpy().reshape(-1,1)\n",
    "    test_y=test['return'].to_numpy().reshape(-1,1)\n",
    "    val_y=val['return'].to_numpy().reshape(-1,1)\n",
    "\n",
    "    feature_scaler=MinMaxScaler()\n",
    "    scale_train_data=feature_scaler.fit_transform(train)\n",
    "    scale_test_data= feature_scaler.transform(test)\n",
    "    scale_val_data=feature_scaler.transform(val)\n",
    "\n",
    "    Label_scaler = MinMaxScaler()\n",
    "    scaled_train_y=Label_scaler.fit_transform(train_y)\n",
    "    scaled_test_y=Label_scaler.transform(test_y)\n",
    "    scaler_val_y=Label_scaler.transform(val_y)\n",
    "\n",
    "    train_X, train_y = timestep_matrix(scale_train_data,scaled_train_y, timestep)\n",
    "    test_X, test_y = timestep_matrix(scale_test_data,scaled_test_y, timestep)\n",
    "    val_X, val_y=timestep_matrix(scale_val_data,scaler_val_y, timestep)\n",
    "\n",
    "    return train_X, test_X, train_y, test_y,val_X, val_y,Label_scaler"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "outputs": [],
   "source": [
    "filepath='./../data/processed_data.csv'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "outputs": [],
   "source": [
    "def get_data(filepath, g_lag, tv_lag, tw_lag,v_lag, timestep, shuffle_times, split_ratio, feature_conditions, month, flag):\n",
    "    # import data\n",
    "    # import the original data. processed_data\n",
    "    # processed_data: weighted reddit score+ fill the nan data\n",
    "    data=pd.read_csv(filepath)\n",
    "    # show the data\n",
    "    data=rename_datetime(data)\n",
    "\n",
    "    # get the subdata set\n",
    "    if flag==0:\n",
    "        data=data[data.index.month==month]\n",
    "    else:\n",
    "        data=data[(data.index.month>=1) & (data.index.month <=month)]\n",
    "\n",
    "    # create features\n",
    "    data_created = create_features(data,g_lag,tv_lag,tw_lag,v_lag)\n",
    "\n",
    "    # keep the wanted features\n",
    "    features = keep_features(feature_conditions)\n",
    "    data=data_created[features]\n",
    "\n",
    "    # split the data\n",
    "    train_val_size=int(len(data)*split_ratio)\n",
    "    train_size=int(train_val_size*split_ratio)\n",
    "    val_size=train_val_size-train_size\n",
    "    test_size=len(data)-train_val_size\n",
    "\n",
    "    train ,val, test=data[0:train_size],data[train_size:train_val_size], data[train_val_size:len(data)]\n",
    "\n",
    "    # normalized the data using MinMaxscaler\n",
    "    n_features=len(features)\n",
    "\n",
    "    train_X, test_X, train_y, test_y,val_X, val_y,Label_scaler =normalize_reshape_data(train, test, val, timestep)\n",
    "\n",
    "    print(\"train_X Shape:\", train_X.shape)\n",
    "    print(\"train_y Shape:\", train_y.shape)\n",
    "    print(\"test_X Shape:\", test_X.shape)\n",
    "    print(\"test_y Shape:\", test_y.shape)\n",
    "\n",
    "    return data, train_X, test_X, train_y, test_y,val_X, val_y, n_features,Label_scaler"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "outputs": [],
   "source": [
    "# # get correlation matrix\n",
    "# sns.heatmap(data.corr(), annot=True)\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Building"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "outputs": [],
   "source": [
    "def create_model(neurons, epochs, dropout, batch_size, verbose, layers,\n",
    "                 activ_func, activ_dense,my_optimizer,\n",
    "                 train_X, train_y, test_X, test_y,val_X, val_y, n_features, timestep,Label_scaler ):\n",
    "    #set seed to reproduce results\n",
    "    np.random.seed(1)\n",
    "    tf.random.set_seed(1)\n",
    "\n",
    "    # design network\n",
    "    model = Sequential()\n",
    "\n",
    "    #return sequences flag if there are more than 1 layer\n",
    "    return_seq = layers > 1\n",
    "\n",
    "    #add first layer\n",
    "    model.add(LSTM(neurons, return_sequences=return_seq, input_shape=(timestep, n_features), activation=activ_func))\n",
    "    # model.add(Dropout(dropout))\n",
    "\n",
    "    #add the other layers\n",
    "    for i in range(1, layers):\n",
    "        ret_seq = i != (layers-1)\n",
    "        model.add(LSTM(neurons, return_sequences=ret_seq, activation=activ_func))\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "    #add a dense layer to output the prediction\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer=my_optimizer)\n",
    "\n",
    "    callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience =50)\n",
    "\n",
    "    # fit network\n",
    "    history = model.fit(train_X, train_y, epochs=epochs, batch_size=batch_size, verbose=verbose,validation_data=(val_X, val_y),callbacks=[callback])\n",
    "\n",
    "    #make prediction\n",
    "    pred = model.predict(test_X)\n",
    "\n",
    "    # Transform back to original form\n",
    "    y_pred=Label_scaler.inverse_transform(pred)\n",
    "\n",
    "    y_true=Label_scaler.inverse_transform(test_y.reshape(-1,1))\n",
    "\n",
    "    RMSE=math.sqrt(mean_squared_error(y_true,y_pred))\n",
    "    MAPE=np.mean(np.abs(y_true-y_pred)/np.abs(y_true))\n",
    "    R2_score =r2_score(y_true, y_pred)\n",
    "\n",
    "    return RMSE, MAPE, R2_score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "outputs": [],
   "source": [
    "# model parameters\n",
    "\n",
    "feature_conditions = {\n",
    "        'google_trends': 0, 'google_trends_lag': 0,\n",
    "        'tweet_volume_lag': 1, 'tw_polarity_lag': 0, 'tw_compound': 0,\n",
    "        'tw_polarity': 0, 'tweet_volume': 0,'re_compound': 0,'re_polarity': 0,\n",
    "        're_subjectivity': 0, 'volume_lag':0\n",
    "    }\n",
    "def test_model(filepath_out, feature_conditions):\n",
    "    columns = [\"timestep\",\"features\",\"google_trends_lag\",\"tweet_volume_lag\",\"tweet_polarity_score_lag\", \"batch_size\", \"neurons\", \"layers\", \"mean_mape\",\"mean_r2\", \"mean_rmse\",\"min_rmse\", \"max_rmse\", \"diff_rmse\",\"optimizer\",\"month\",\"consecutive\",\"actic_func\"]\n",
    "\n",
    "    try:\n",
    "        results = pd.read_csv(filepath_out)\n",
    "    except:\n",
    "        results = pd.DataFrame(columns=columns)\n",
    "\n",
    "    #lagged_features\n",
    "    timestep = [15]\n",
    "    #train_ratio\n",
    "    split_ratio =0.8\n",
    "    shuffle_times = 3\n",
    "    activ_func = \"relu\"\n",
    "    activ_dense = 'sigmoid'\n",
    "    my_optimizer = 'adam'\n",
    "    # my_optimizer=RMSprop(lr=0.01, rho=0.9, epsilon=None, decay=0.0)\n",
    "    epochs = 10000\n",
    "    months =[6]\n",
    "     #for each lag feature\n",
    "    for month in tqdm(months,total=len(months)):\n",
    "        for step in tqdm(timestep,total=len(timestep)):\n",
    "            neurons = [32]\n",
    "            layers = [1]\n",
    "            batch_sizes = [80]\n",
    "            dropout = 0.25\n",
    "            verbose=0\n",
    "            # Lags\n",
    "            g_lag = [2]\n",
    "            tv_lag = 31 # tweets volume\n",
    "            tw_lag = 15 # tweets score\n",
    "            v_lag= [3]\n",
    "\n",
    "            # set flag: 1 - consecutive months, flag: 0 - single month\n",
    "            flag=1\n",
    "\n",
    "            #for each epoch, neuron, layers and batch_size value\n",
    "            for n in neurons:\n",
    "                for l in layers:\n",
    "                    for b in  batch_sizes:\n",
    "                        print(\"Testing model: lag:\", step, \", neurons:\", n, \", layers:\", l, \", batch_size:\", b)\n",
    "                        for g in g_lag:\n",
    "                            for v in v_lag:\n",
    "                                #run for 5 times\n",
    "                                rmse = []\n",
    "                                mape =[]\n",
    "                                r2 = []\n",
    "                                for i in tqdm(range (0,5)):\n",
    "                                    data, train_X, test_X, train_y, test_y,val_X, val_y, n_features,Label_scaler = get_data(filepath, g, tv_lag, tw_lag,v, step, shuffle_times, split_ratio, feature_conditions,month,flag)\n",
    "                                    RMSE, MAPE, R2 = create_model(n, epochs, dropout, b, verbose, l, activ_func, activ_dense,my_optimizer, train_X, train_y, test_X, test_y, val_X, val_y, n_features, step,Label_scaler)\n",
    "                                    rmse.append(RMSE)\n",
    "                                    mape.append(MAPE)\n",
    "                                    r2.append(R2)\n",
    "\n",
    "                                #calculate mean values\n",
    "                                rmse = np.array(rmse)\n",
    "                                mean_rmse =rmse.mean()\n",
    "                                min_rmse =rmse.min()\n",
    "                                max_rmse =rmse.max()\n",
    "                                diff_rmse = max_rmse - min_rmse\n",
    "                                mean_mape= np.array(mape).mean()\n",
    "                                mean_r2=np.array(r2).mean()\n",
    "\n",
    "                                results = results.append({\"timestep\": step,\"features\": data.columns.values,\"google_trends_lag\":g,\"tweet_volume_lag\": tv_lag,\"tweet_polarity_score_lag\": tw_lag, \"batch_size\":b, \"neurons\":n, \"layers\":l, \"mean_mape\":mean_mape,\"mean_r2\":mean_r2, \"mean_rmse\": mean_rmse,\"min_rmse\":min_rmse, \"max_rmse\":max_rmse, \"diff_rmse\": diff_rmse,\"optimizer\":my_optimizer,\"month\":month,\"consecutive\": flag,\"actic_func\":activ_func}, ignore_index=True)\n",
    "    return pd.DataFrame(results)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model: lag: 15 , neurons: 32 , layers: 1 , batch_size: 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001B[A\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X Shape: (2744, 15, 2)\n",
      "train_y Shape: (2744, 1)\n",
      "test_X Shape: (847, 15, 2)\n",
      "test_y Shape: (847, 1)\n",
      "27/27 [==============================] - 0s 922us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 20%|██        | 1/5 [00:25<01:43, 25.96s/it]\u001B[A\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X Shape: (2744, 15, 2)\n",
      "train_y Shape: (2744, 1)\n",
      "test_X Shape: (847, 15, 2)\n",
      "test_y Shape: (847, 1)\n",
      "27/27 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 40%|████      | 2/5 [00:55<01:24, 28.13s/it]\u001B[A\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X Shape: (2744, 15, 2)\n",
      "train_y Shape: (2744, 1)\n",
      "test_X Shape: (847, 15, 2)\n",
      "test_y Shape: (847, 1)\n",
      "27/27 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 60%|██████    | 3/5 [01:21<00:53, 26.92s/it]\u001B[A\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X Shape: (2744, 15, 2)\n",
      "train_y Shape: (2744, 1)\n",
      "test_X Shape: (847, 15, 2)\n",
      "test_y Shape: (847, 1)\n",
      "27/27 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 80%|████████  | 4/5 [01:37<00:22, 22.75s/it]\u001B[A\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X Shape: (2744, 15, 2)\n",
      "train_y Shape: (2744, 1)\n",
      "test_X Shape: (847, 15, 2)\n",
      "test_y Shape: (847, 1)\n",
      "27/27 [==============================] - 0s 910us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|██████████| 5/5 [01:58<00:00, 23.79s/it]\u001B[A\u001B[A\n",
      "/var/folders/vp/8_rd4_c56g3dpqb1167z6tqr0000gn/T/ipykernel_26242/2550342164.py:72: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({\"timestep\": step,\"features\": data.columns.values,\"google_trends_lag\":g,\"tweet_volume_lag\": tv_lag,\"tweet_polarity_score_lag\": tw_lag, \"batch_size\":b, \"neurons\":n, \"layers\":l, \"mean_mape\":mean_mape,\"mean_r2\":mean_r2, \"mean_rmse\": mean_rmse,\"min_rmse\":min_rmse, \"max_rmse\":max_rmse, \"diff_rmse\": diff_rmse,\"optimizer\":my_optimizer,\"month\":month,\"consecutive\": flag,\"actic_func\":activ_func}, ignore_index=True)\n",
      "\n",
      "100%|██████████| 1/1 [01:58<00:00, 118.98s/it]\u001B[A\n",
      "100%|██████████| 1/1 [01:58<00:00, 118.98s/it]\n"
     ]
    }
   ],
   "source": [
    "filepath_out='./../data/lstm_returns_regression.csv'\n",
    "results=test_model(filepath_out, feature_conditions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "outputs": [],
   "source": [
    "results.to_csv(filepath_out, index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}