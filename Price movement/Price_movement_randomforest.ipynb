{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Regression Model for price movement"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import datetime\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import keras.utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from sklearn.metrics import classification_report"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def rename_datetime(data):\n",
    "    data.rename(columns={'Unnamed: 0': 'datetime'},inplace=True)\n",
    "    data['datetime']=pd.to_datetime(data['datetime'])\n",
    "    data.set_index('datetime', inplace=True)\n",
    "    return data\n",
    "# feature selection\n",
    "# create features\n",
    "def create_features(data, g_lag, tv_lag, tw_lag):\n",
    "    data['Change'] =data['Close'].diff().dropna()\n",
    "    data['Label'] = np.where(data['Change']>0, 1 ,0)\n",
    "    data['google_trends_lag']=data['google_trends'].shift(g_lag)\n",
    "    data['tweet_volume_lag']=data['tweet_volume'].shift(tv_lag)\n",
    "    data['tw_polarity_lag'] = data['tw_polarity'].shift(tw_lag)\n",
    "\n",
    "    data.drop(columns=['Open','High','Low','Change'],inplace=True)\n",
    "    # Add features like RSI? Moving average?\n",
    "\n",
    "    data.dropna(inplace=True)\n",
    "    return data\n",
    "# keep the wanted features\n",
    "def keep_features(feature_conditions):\n",
    "    features=['Label','Close']\n",
    "    for feature, condition in feature_conditions.items():\n",
    "        if condition:\n",
    "            features.append(feature)\n",
    "    return features\n",
    "\n",
    "# create the lagged features based on the timesteps\n",
    "def reshape_features(data, to_keep=1, to_remove=1):\n",
    "    variables = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    columns, names = list(), list()\n",
    "\n",
    "    for i in range(to_keep, 0, -1):\n",
    "        columns.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(variables)]\n",
    "\n",
    "    for i in range(0, to_remove):\n",
    "        columns.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(variables)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(variables)]\n",
    "\n",
    "    #put it all together\n",
    "    final = pd.concat(columns, axis=1)\n",
    "    final.columns = names\n",
    "\n",
    "    #drop rows with NaN values\n",
    "    final.dropna(inplace=True)\n",
    "\n",
    "    new_data = final.reset_index()\n",
    "\n",
    "    new_data = new_data.drop(columns=['datetime'])\n",
    "\n",
    "    return new_data\n",
    "\n",
    "# shuffle the data\n",
    "def shuffle_data(times, data):\n",
    "    np.random.seed(1)\n",
    "    for i in range(times+1):\n",
    "        data=shuffle(data)\n",
    "    return data\n",
    "\n",
    "# split labels from data\n",
    "def split_label(train, test):\n",
    "    train_y = train['var1(t)'].values\n",
    "    test_y = test['var1(t)'].values\n",
    "    train_y = train_y.reshape(len(train_y), 1)\n",
    "    test_y = test_y.reshape(len(test_y), 1)\n",
    "    return train_y, test_y\n",
    "\n",
    "# normalize data using Minmaxscaler\n",
    "def normalize_reshape_data(train, test, train_y, test_y, all_features, n_features, timestep):\n",
    "    feature_scaler=MinMaxScaler()\n",
    "    scale_train_data=feature_scaler.fit_transform(train)\n",
    "    scale_test_data= feature_scaler.transform(test)\n",
    "    train = scale_train_data[:, :all_features]\n",
    "    test = scale_test_data[:, :all_features]\n",
    "    #keep only prices array\n",
    "    train_X, train_y = train[:, :all_features], train_y[:, -1]\n",
    "    test_X, test_y = test[:, :all_features], test_y[:, -1]\n",
    "\n",
    "    return train_X, test_X, train_y, test_y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "filepath='./../data/processed_data.csv'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def get_data(filepath, g_lag, tv_lag, tw_lag, timestep, shuffle_times, split_ratio, feature_conditions):\n",
    "    # import data\n",
    "    # import the original data. processed_data\n",
    "    # processed_data: weighted reddit score+ fill the nan data\n",
    "    data=pd.read_csv(filepath)\n",
    "    # show the data\n",
    "    data=rename_datetime(data)\n",
    "\n",
    "    # create features\n",
    "    data_created = create_features(data,g_lag,tv_lag,tw_lag)\n",
    "\n",
    "    # keep the wanted features\n",
    "    features = keep_features(feature_conditions)\n",
    "    data=data_created[features]\n",
    "\n",
    "    # reshape the data\n",
    "    # create the lagged features based on the timesteps\n",
    "    df_copy = data.copy()\n",
    "    new_data=reshape_features(df_copy, timestep, 1)\n",
    "\n",
    "    # shuffle the data\n",
    "    shuffled_data=shuffle_data(shuffle_times, new_data)\n",
    "\n",
    "    # split the data\n",
    "    train, test= train_test_split(shuffled_data, test_size=split_ratio)\n",
    "    train_y, test_y=split_label(train, test)\n",
    "\n",
    "    # normalized the data using MinMaxscaler\n",
    "    n_features=len(features)\n",
    "    all_features = timestep * n_features\n",
    "    if (all_features==0):\n",
    "        all_features=n_features\n",
    "    train_X, test_X, train_y, test_y =normalize_reshape_data(train, test, train_y, test_y, all_features, n_features,timestep)\n",
    "\n",
    "    print(\"train_X Shape:\", train_X.shape)\n",
    "    print(\"train_y Shape:\", train_y.shape)\n",
    "    print(\"test_X Shape:\", test_X.shape)\n",
    "    print(\"test_y Shape:\", test_y.shape)\n",
    "\n",
    "    return data, train_X, test_X, train_y, test_y, n_features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# # get correlation matrix\n",
    "# sns.heatmap(data.corr(), annot=True)\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Building"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def create_model(train_X, train_y, test_X, test_y, n_features, timestep ):\n",
    "    #set seed to reproduce results\n",
    "    np.random.seed(1)\n",
    "    tf.random.set_seed(1)\n",
    "\n",
    "    # set the range of the hyperparameters\n",
    "    param_grid = {\n",
    "        'n_estimators': np.arange(100,1000,100),\n",
    "         'max_depth': [10,20,30],\n",
    "    }\n",
    "    # Create a Random Forest Classifier\n",
    "    forest = RandomForestClassifier(oob_score = True, criterion = \"gini\", random_state = 18)\n",
    "\n",
    "    # hyperparameter selection with GridSearchCV\n",
    "    grid_search = GridSearchCV(forest, param_grid=param_grid, cv=5, scoring='accuracy', refit=True,n_jobs=-1, return_train_score=True)\n",
    "    grid_search.fit(train_X, train_y)\n",
    "\n",
    "    forest_best = grid_search.best_estimator_\n",
    "    print(forest_best)\n",
    "    # Make predictions\n",
    "    y_pred = forest_best.predict(test_X)\n",
    "    results=grid_search.cv_results_\n",
    "    # calculate the metrics\n",
    "    report=classification_report(\n",
    "          test_y,\n",
    "          y_pred,target_names = [\"Down\", \"Up\"],\n",
    "          digits = 5, output_dict=True)\n",
    "\n",
    "    # precision = report['Down']['precision']\n",
    "    down_f1_score = report['Down']['f1-score']\n",
    "    up_f1_score = report['Up']['f1-score']\n",
    "    accuracy=report['accuracy']\n",
    "\n",
    "    return down_f1_score, up_f1_score, accuracy, grid_search.best_params_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# model parameters\n",
    "\n",
    "feature_conditions = {\n",
    "        'google_trends': 0, 'google_trends_lag': 0,\n",
    "        'tweet_volume_lag': 0, 'tw_polarity_lag': 0, 'tw_compound': 0,\n",
    "        'tw_polarity': 0, 'tweet_volume': 0,'re_compound': 0,'re_polarity': 0,\n",
    "        're_subjectivity': 0\n",
    "    }\n",
    "def test_model(filepath_out, feature_conditions):\n",
    "    columns = [\"timestep\",\"features\",\"google_trends_lag\",\"tweet_volume_lag\",\"tweet_polarity_score_lag\",\"best_estimators\", \"down_f1_score\",\"up_f1_score\",\"acc\"]\n",
    "\n",
    "    try:\n",
    "        results = pd.read_csv(filepath_out)\n",
    "    except:\n",
    "        results = pd.DataFrame(columns=columns)\n",
    "\n",
    "    #lagged_features\n",
    "    timestep = [10, 15]\n",
    "    #train_ratio\n",
    "    split_ratio =0.2\n",
    "    shuffle_times = 3\n",
    "\n",
    "     #for each lag feature\n",
    "    for step in timestep:\n",
    "        # Lags\n",
    "        g_lag = [10]\n",
    "        tv_lag = [28] # tweets volume\n",
    "        tw_lag = 15 # tweets score\n",
    "\n",
    "\n",
    "\n",
    "        for g in g_lag:\n",
    "            for tv in tv_lag:\n",
    "\n",
    "                data, train_X, test_X, train_y, test_y, n_features = get_data(filepath, g, tv, tw_lag, step, shuffle_times, split_ratio, feature_conditions)\n",
    "                down_score, up_score, accuracy,best_estimators  = create_model(train_X, train_y, test_X, test_y, n_features, step)\n",
    "\n",
    "\n",
    "\n",
    "                results = results.append({\"timestep\": step,\"features\": data.columns.values,\"google_trends_lag\":g,\"tweet_volume_lag\": tv,\"tweet_polarity_score_lag\": tw_lag, \"best_estimators\":best_estimators,\"down_f1_score\":down_score,\"up_f1_score\":up_score, \"acc\": accuracy}, ignore_index=True)\n",
    "    return pd.DataFrame(results)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X Shape: (3444, 20)\n",
      "train_y Shape: (3444,)\n",
      "test_X Shape: (862, 20)\n",
      "test_y Shape: (862,)\n",
      "RandomForestClassifier(max_depth=10, n_estimators=300, oob_score=True,\n",
      "                       random_state=18)\n",
      "train_X Shape: (3440, 30)\n",
      "train_y Shape: (3440,)\n",
      "test_X Shape: (861, 30)\n",
      "test_y Shape: (861,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vp/8_rd4_c56g3dpqb1167z6tqr0000gn/T/ipykernel_27864/3778345885.py:40: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({\"timestep\": step,\"features\": data.columns.values,\"google_trends_lag\":g,\"tweet_volume_lag\": tv,\"tweet_polarity_score_lag\": tw_lag, \"best_estimators\":best_estimators,\"down_f1_score\":down_score,\"up_f1_score\":up_score, \"acc\": accuracy}, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(max_depth=10, n_estimators=300, oob_score=True,\n",
      "                       random_state=18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vp/8_rd4_c56g3dpqb1167z6tqr0000gn/T/ipykernel_27864/3778345885.py:40: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({\"timestep\": step,\"features\": data.columns.values,\"google_trends_lag\":g,\"tweet_volume_lag\": tv,\"tweet_polarity_score_lag\": tw_lag, \"best_estimators\":best_estimators,\"down_f1_score\":down_score,\"up_f1_score\":up_score, \"acc\": accuracy}, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "filepath_out='./../data/randomforest_results_1.csv'\n",
    "results=test_model(filepath_out, feature_conditions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "results.to_csv(filepath_out, index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}